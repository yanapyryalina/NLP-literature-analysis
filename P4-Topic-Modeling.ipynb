{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Literature Analysis\n",
    "\n",
    "### Reading is great. And with so many amazing books out there also come great movies, reviews, and summaries. Reading those reviews and watching those films often only gives us a picture of what the book is actually like, though. With the power of data science and natural language processing, I am able to bring another dimension to how we understand literature.\n",
    "\n",
    "For this project, I am looking at the following eight writings:\n",
    "* **The Foundation by Isaac Asimov** - a book I am currently reading, by my favorite sci-fi writer \n",
    "* **A Clockwork Orange by Anthony Burgess** - the writing behind a famous extravagant horror movie by Stanley Kubrik, a book with a unique writing style and vocabulary\n",
    "* **Comments to the Society of the Spectacle by Guy Debord** - a continuation of a book I was taught in university about the influence of the capitalist media on the society\n",
    "* **A Brief History of Time by Stephen Hawking** - a book that excited millions about the workings of our universe\n",
    "* **For Whom the Bell Tolls by Ernest Hemingway** - a writing with a unique writing style and themes specific to American writers\n",
    "* **Carrie by Stephen King** - one of the most well-known horrors out there\n",
    "* **The Hobbit by J.R.R. Tolkien** - a very long journey by very short people, one that so many people and communities hold dear to their heart\n",
    "* **Slaughterhouse Five by Kurt Vonnegut** - a book highly recommended to me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will be covering the steps on how to do **Latent Dirichlet Allocation (LDA)**, which is one of many topic modeling techniques. It was specifically designed for text data.\n",
    "\n",
    "To use a topic modeling technique, you need to provide (1) a document-term matrix and (2) the number of topics you would like the algorithm to pick up.\n",
    "\n",
    "Once the topic modeling technique is applied, your job as a human is to interpret the results and see if the mix of words in each topic make sense. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *I found LDA to be a good choice of tool for this project due to the interpretability of topics. Working with large and complex pieces of data like literature works, I wanted to see if I as a reader could pick up latent/hidden topics as a result of my data analysis, which was successful (as shown in the Insights section)*\n",
    "\n",
    "\n",
    "1. Topic modeling based on the **entire** original document-term matrix (all parts of speech)\n",
    "    - Input the document-term matrix, transpose, and transform it into gensim corpus required by the LDA (Latent Dirichlet Allocation)\n",
    "    - Create a dictionary of all terms and their respective location in the term-document matrix\n",
    "    - Specify number of topics and passes, and run the LDA model\n",
    "    \n",
    "    \n",
    "2. **Extract Parts of Speech** for topic model\n",
    "    - *I chose to use parts-of-speech extraction mainly to improve the results of my LDA model*\n",
    "    - **Nouns only**\n",
    "        - *Filtering and using only nouns for our LDA model could be an improvement simply because topics themselves (like “war” or “love”) are nouns. For example, in the sentence “The war was dreadful and unforgiving, but it could not destroy their love” we could simply extract nouns “war” and “love”, showing the sentence’s themes much clearer.*\n",
    "\n",
    "        - Create a function to tokenize given text and extract only the nouns\n",
    "        - Input the clean data\n",
    "        - Apply the noun-filtering function\n",
    "        - Create a new document-term matrix using only nouns\n",
    "        - Create a new gensim corpus (based on the new document-term matrix)\n",
    "        - Create a new vocabulary vocabulary dictionary\n",
    "        - Test the LDA model, gradually increasing the number of topics\n",
    "            - *Starting off with fewer topics and gradually increasing their number helped me track down at what point my LDA model starts giving redundant results (at what point and how its performance deteriorates) so that I can see what to fix*\n",
    "     \n",
    "    - **Nouns and adjectives**\n",
    "        - Repeat the above process with nouns & adjectives\n",
    "            - *At the same time, adjectives could also be useful for making a better term-document matrix for our LDA model. For example, in the sentence “Her face was bloody and demonic”, words like “bloody” and “demonic” hint at the themes much better than the word “face”.*\n",
    "        \n",
    "    - **Final Model**\n",
    "        - Take the most recent noun+adjective function, set the topic number to 5 and pass number to 100\n",
    "            - *After gradually increasing the number of topics and number of times for the model to run, 5 topics and 100 passes ended up giving topic clusters that made sense the most in connection to the books’ known themes*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling - Attempt #1 (All Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's read in our document-term matrix\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "data = pd.read_pickle('dtm_stop.pkl')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules for LDA with gensim\n",
    "# Terminal / Anaconda Navigator: conda install -c conda-forge gensim\n",
    "from gensim import matutils, models\n",
    "import scipy.sparse\n",
    "\n",
    "# Optional:\n",
    "# import logging # helps debug - logs every step\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One of the required inputs is a term-document matrix\n",
    "tdm = data.transpose() \n",
    "tdm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to put the term-document matrix into a new gensim format, from df --> sparse matrix --> gensim corpus\n",
    "sparse_counts = scipy.sparse.csr_matrix(tdm) # create a sparse matrix from the term-document matrix\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts) # create a gensim corpus from the sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim also requires dictionary of the all terms and their respective location in the term-document matrix\n",
    "cv = pickle.load(open(\"cv_stop.pkl\", \"rb\")) # load the pickled count vectorizer object\n",
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items()) # create a dictionary with location keys and word values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term), we need to specify two other parameters - the number of topics and the number of passes. Let's start the number of topics at 2, see if the results make sense, and increase the number from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify two other parameters - the number of topics and the number of passes\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10)\n",
    "lda.print_topics()\n",
    "\n",
    "# Oh wow! I expected it to do much worse!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA for num_topics = 3\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=3, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA for num_topics = 4\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=4, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found this model to be a decent start. However, the results sometimes have quite a few verbs (*come, thought, dont, going, went, got*) that clutter-up our word groups - the verbs don’t really mean much, and if they had not been there, our results might have been more comprehensive. That is why I chose to filter out parts of speech and see if my models could be improved with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling - Attempt #2 (Nouns only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "def nouns(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns.'''\n",
    "    is_noun = lambda pos: pos[:2] == 'NN' # lambda function determining if part of speech is noun\n",
    "    tokenized = word_tokenize(text) # tokenize the text\n",
    "    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)] # if the word is noun in the tokenized text, add the word to the list of nouns\n",
    "    return ' '.join(all_nouns) # return a string with all elements of noun list joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the cleaned data, before the CountVectorizer step\n",
    "data_clean = pd.read_pickle('data_clean.pkl')\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the nouns function to the writings to filter only on nouns\n",
    "data_nouns = pd.DataFrame(data_clean.writing.apply(nouns))\n",
    "data_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new document-term matrix using only nouns\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Add the additional stop words since we are recreating the document-term matrix\n",
    "add_stop_words = ['chapter', 'im', 'know', 'just', 'dont', 'thats', 'right', 'people',\n",
    "                  'youre', 'got', 'gonna', 'think', 'yeah', 'said', 'i']\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
    "\n",
    "# Recreate a document-term matrix with only nouns\n",
    "cvn = CountVectorizer(stop_words=stop_words) # create a count vectorizer for nouns excluding stop words\n",
    "data_cvn = cvn.fit_transform(data_nouns.writing) # fit the vectorizer onto the data\n",
    "data_dtmn = pd.DataFrame(data_cvn.toarray(), columns=cvn.get_feature_names()) # convert into a 2D array\n",
    "data_dtmn.index = data_nouns.index # label the columns\n",
    "data_dtmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-create the gensim corpus\n",
    "corpusn = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmn.transpose()))\n",
    "\n",
    "# Re-create the vocabulary dictionary\n",
    "id2wordn = dict((v, k) for k, v in cvn.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with 2 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=2, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try topics = 3\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=3, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()\n",
    "\n",
    "# My first attempt to run this cell yielded pretty good results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try 4 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=4, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()\n",
    "\n",
    "# However, after 3 topics the program started getting a bit more confused again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling - Attempt #3 (Nouns and Adjectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns and adjectives from a string of text\n",
    "def nouns_adj(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives.'''\n",
    "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ' # lambda function determining if part of speech is noun or adjective\n",
    "    tokenized = word_tokenize(text) # tokenize the text\n",
    "    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] # if the word is noun/adjective in the tokenized text, add the word to the list of nouns&adjectives\n",
    "    return ' '.join(nouns_adj) # return a string with all elements of noun&adjective list joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the books to filter nouns and adjectives\n",
    "data_nouns_adj = pd.DataFrame(data_clean.writing.apply(nouns_adj))\n",
    "data_nouns_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new document-term matrix using only nouns and adjectives, also remove common words with max_df\n",
    "cvna = CountVectorizer(stop_words=stop_words, max_df=.8)\n",
    "data_cvna = cvna.fit_transform(data_nouns_adj.writing)\n",
    "data_dtmna = pd.DataFrame(data_cvna.toarray(), columns=cvna.get_feature_names())\n",
    "data_dtmna.index = data_nouns_adj.index\n",
    "data_dtmna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-create the gensim corpus\n",
    "corpusna = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmna.transpose()))\n",
    "\n",
    "# Re-create the vocabulary dictionary\n",
    "id2wordna = dict((v, k) for k, v in cvna.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with 2 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=2, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try 3 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=3, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try 4 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try 5 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=5, id2word=id2wordna, passes=30)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the 9 topic models we looked at, the nouns and adjectives, 5 topic one made the most sense. So let's pull that down here and run it through some more iterations to get more fine-tuned topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our final LDA model (for now)\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=5, id2word=id2wordna, passes=100)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result I got:\n",
    "    \n",
    " (0,\n",
    "  '0.009 * \"carrie\" + 0.006 * \"brothers\" + 0.004 * \"momma\" + 0.004 * \"veck\" + 0.004 * \"door\" + 0.004 * \"school\" + 0.003 * \"dim\" + 0.003 * \"bed\" + 0.003 * \"horrorshow\" + 0.003 * \"malenky\"'),\n",
    "  \n",
    " (1,\n",
    "  '0.013 * \"jordan\" + 0.012 * \"bilbo\" + 0.010 * \"robert\" + 0.009 * \"dwarves\" + 0.006 * \"pilar\" + 0.006 * \"road\" + 0.006 * \"thee\" + 0.006 * \"pablo\" + 0.006 * \"bridge\" + 0.006 * \"thorin\"'),\n",
    "  \n",
    " (2,\n",
    "  '0.010 * \"spectacle\" + 0.006 * \"spectacular\" + 0.003 * \"social\" + 0.003 * \"media\" + 0.003 * \"disinformation\" + 0.003 * \"false\" + 0.002 * \"mafia\" + 0.002 * \"services\" + 0.002 * \"information\" + 0.002 * \"example\"'),\n",
    "  \n",
    " (3,\n",
    "  '0.018 * \"universe\" + 0.009 * \"theory\" + 0.008 * \"foundation\" + 0.007 * \"brief\" + 0.006 * \"particles\" + 0.006 * \"hardin\" + 0.005 * \"energy\" + 0.005 * \"mallow\" + 0.004 * \"seldon\" + 0.004 * \"star\"'),\n",
    "  \n",
    " (4,\n",
    "  '0.000 * \"carrie\" + 0.000 * \"jordan\" + 0.000 * \"robert\" + 0.000 * \"road\" + 0.000 * \"thee\" + 0.000 * \"momma\" + 0.000 * \"universe\" + 0.000 * \"bilbo\" + 0.000 * \"door\" + 0.000 * \"brothers\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, the final model with 5 topics and 100 passes yielded much better results than the mini-noun-adjective model and even the noun-only model. As we can see above, the results include more comprehensive words (including adjectives), take words from more books, and incorporate a much wider variety of topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a long long time of iterating our LDA model, I ended up with five topics that look pretty decent. Let's settle on these for now.\n",
    "* **Topic 1: horror:** carrie, brothers, momma, school, horrorshow\n",
    "* **Topic 2: hobbit's world:** bilbo, dwarves, three, goblin, mountain, hobbit\n",
    "* **Topic 3: media:** spectacle, disinformation, false, mafia, services\n",
    "* **Topic 4: outer space:** universe, theory, foundation, particles, hardin, energy, star\n",
    "* **Topic 5: the way somewhere:** bridge, road, door"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, the topics seem to make sense, as they are mainly divided by genres like sci-fi, horror, fantasy, non-fiction. However, in addition to books overlapping in their genres, one major visible overlap between all books seems to be “the way somewhere”. I find this interesting because while for humans concept of journey in stories makes sense, a computer may not have a general assumption that books (even non-fiction or scientific reports) are overall descriptions of some kind of journey (*road*) through obstacles (*bridge, door*) rather than random collections of facts or events. Through such relatively simple data analysis, a program can have a chance to gain a deeper understanding of literature - its essence and (in the future) possibly even its appeal. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next up - Text Generation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
